# -*- coding: utf-8 -*-
"""Image Captioning using CNN + LSTM (Deep Learning).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pKYgrwW1BbCxeOcU-7lO2w31ebT7-_yO
"""

# image_captioning.py

import numpy as np
import matplotlib.pyplot as plt
import os
import string
from tqdm import tqdm
import pickle
from PIL import Image

import tensorflow as tf
from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add
from tensorflow.keras.models import Model

# Set seed
np.random.seed(42)
tf.random.set_seed(42)

# Load and preprocess images
def preprocess_image(img_path):
    img = load_img(img_path, target_size=(299, 299))
    img = img_to_array(img)
    img = preprocess_input(img)
    return img

def encode_image(model, image_path):
    image = preprocess_image(image_path)
    image = np.expand_dims(image, axis=0)
    feature = model.predict(image)
    feature = np.reshape(feature, feature.shape[1])
    return feature

# Load dataset
def load_descriptions(file_path):
    with open(file_path, 'r') as f:
        text = f.read()

    descriptions = {}
    for line in text.strip().split('\n'):
        tokens = line.split('\t')
        if len(tokens) == 2:
            image_id, caption = tokens
            image_id = image_id.split('.')[0]
            if image_id not in descriptions:
                descriptions[image_id] = []
            descriptions[image_id].append('startseq ' + caption + ' endseq')
    return descriptions

# Create tokenizer
def create_tokenizer(descriptions):
    all_captions = []
    for key in descriptions.keys():
        all_captions.extend(descriptions[key])
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(all_captions)
    return tokenizer

# Create sequences
def create_sequences(tokenizer, max_length, descriptions, features, vocab_size):
    X1, X2, y = [], [], []
    for key, desc_list in descriptions.items():
        for desc in desc_list:
            seq = tokenizer.texts_to_sequences([desc])[0]
            for i in range(1, len(seq)):
                in_seq, out_seq = seq[:i], seq[i]
                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
                out_seq = tf.keras.utils.to_categorical([out_seq], num_classes=vocab_size)[0]
                X1.append(features[key])
                X2.append(in_seq)
                y.append(out_seq)
    return np.array(X1), np.array(X2), np.array(y)

# Define model
def define_model(vocab_size, max_length):
    inputs1 = Input(shape=(2048,))
    fe1 = Dropout(0.5)(inputs1)
    fe2 = Dense(256, activation='relu')(fe1)

    inputs2 = Input(shape=(max_length,))
    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
    se2 = Dropout(0.5)(se1)
    se3 = LSTM(256)(se2)

    decoder1 = add([fe2, se3])
    decoder2 = Dense(256, activation='relu')(decoder1)
    outputs = Dense(vocab_size, activation='softmax')(decoder2)

    model = Model(inputs=[inputs1, inputs2], outputs=outputs)
    model.compile(loss='categorical_crossentropy', optimizer='adam')
    return model

# Caption generation
def generate_caption(model, tokenizer, photo, max_length):
    in_text = 'startseq'
    for _ in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_length)
        yhat = model.predict([photo, sequence], verbose=0)
        yhat = np.argmax(yhat)
        word = tokenizer.index_word.get(yhat)
        if word is None:
            break
        in_text += ' ' + word
        if word == 'endseq':
            break
    return in_text

# InceptionV3 for image features
def get_feature_extractor():
    base_model = InceptionV3(weights='imagenet')
    model = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)
    return model

# Paths and data
# You must provide:
# - `captions.txt` file (image_id<tab>caption)
# - `Images/` folder with image files
# - OR change the above code to use MS COCO or Flickr8k/30k dataset